{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T14:08:35.755640Z",
     "iopub.status.busy": "2025-01-02T14:08:35.755368Z",
     "iopub.status.idle": "2025-01-02T14:08:50.970917Z",
     "shell.execute_reply": "2025-01-02T14:08:50.970109Z",
     "shell.execute_reply.started": "2025-01-02T14:08:35.755619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in /usr/local/lib/python3.10/dist-packages (2024.12.12)\n",
      "Found existing installation: unsloth 2024.12.12\n",
      "Uninstalling unsloth-2024.12.12:\n",
      "  Successfully uninstalled unsloth-2024.12.12\n",
      "Collecting git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-aiie2a0x\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-aiie2a0x\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 87f5bffc45a8af7f23a41650b30858e097b86418\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for unsloth: filename=unsloth-2024.12.12-py3-none-any.whl size=175166 sha256=3c2dfa9659502a41ccdd0b14633a0795534f9b806283cef14a3e8559faeac29c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ysifr5lj/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2024.12.12\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T14:08:50.972129Z",
     "iopub.status.busy": "2025-01-02T14:08:50.971866Z",
     "iopub.status.idle": "2025-01-02T14:08:50.975993Z",
     "shell.execute_reply": "2025-01-02T14:08:50.975297Z",
     "shell.execute_reply.started": "2025-01-02T14:08:50.972098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T14:10:44.096807Z",
     "iopub.status.busy": "2025-01-02T14:10:44.096506Z",
     "iopub.status.idle": "2025-01-02T14:11:05.565082Z",
     "shell.execute_reply": "2025-01-02T14:11:05.563964Z",
     "shell.execute_reply.started": "2025-01-02T14:10:44.096784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.12: Fast Mistral patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc34b02de204c9d956d77c70007bb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401e2b1005714ffd80367495fd6a8a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/157 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f6d251cfd84cc495197f1ca1990cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bed92ae1b3433c95756cee6f85dbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d0551f7c73456ea3fe60eff07ebcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc828affc884da2b691a56090808232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4092 \n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"            \n",
    "] \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    device_map=\"balanced\",\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T14:11:15.180823Z",
     "iopub.status.busy": "2025-01-02T14:11:15.180498Z",
     "iopub.status.idle": "2025-01-02T14:11:20.788582Z",
     "shell.execute_reply": "2025-01-02T14:11:20.787888Z",
     "shell.execute_reply.started": "2025-01-02T14:11:15.180801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T14:11:20.789789Z",
     "iopub.status.busy": "2025-01-02T14:11:20.789587Z",
     "iopub.status.idle": "2025-01-02T14:11:30.692248Z",
     "shell.execute_reply": "2025-01-02T14:11:30.691393Z",
     "shell.execute_reply.started": "2025-01-02T14:11:20.789771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1537e7ba270e451594ddfa008726767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the alpaca_prompt template\n",
    "alpaca_prompt = \"\"\"\n",
    "System: {0}\n",
    "\n",
    "Instruction: {1}\n",
    "\n",
    "Response: {2}\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and move to GPU\n",
    "inputs = tokenizer(\n",
    "    alpaca_prompt.format(\n",
    "        \"Provide the necessary calculations and reasoning to solve the given problem.\",  # system\n",
    "        \"In a school, there are 400 students. Among them, 60% are boys.Calculate no of boys\",  # instruction\n",
    "        \"\"  # output - leave this blank for generation!\n",
    "    ),\n",
    "    return_tensors=\"pt\"  # Convert to PyTorch tensors\n",
    ").to(\"cuda\")  # Move to GPU\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Ensure EOS_TOKEN is included\n",
    "\n",
    "# Define formatting function\n",
    "def formatting_prompts_func(examples):\n",
    "    systems = examples[\"system\"]          # Fetch 'system' field\n",
    "    instructions = examples[\"instruction\"]  # Fetch 'instruction' field\n",
    "    responses = examples[\"response\"]      # Fetch 'response' field\n",
    "    texts = []\n",
    "    for system, instruction, response in zip(systems, instructions, responses):\n",
    "        # Format using the alpaca_prompt and add EOS_TOKEN\n",
    "        text = alpaca_prompt.format(system, instruction, response) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "from datasets import load_dataset\n",
    "# Load the Synthia dataset\n",
    "dataset = load_dataset(\"migtissera/Tess-v1.5\", split=\"train\")\n",
    "# Apply formatting function\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T14:11:30.694358Z",
     "iopub.status.busy": "2025-01-02T14:11:30.694074Z",
     "iopub.status.idle": "2025-01-02T14:11:32.998798Z",
     "shell.execute_reply": "2025-01-02T14:11:32.997929Z",
     "shell.execute_reply.started": "2025-01-02T14:11:30.694335Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4216.1809\n"
     ]
    }
   ],
   "source": [
    "total_length = 0\n",
    "num_samples = 30000\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    i = np.random.randint(0, 40000)\n",
    "    sample = dataset[i]\n",
    "    total_length += len(sample['system']) + len(sample['instruction']) + len(sample['response'])\n",
    "\n",
    "mean_length = total_length / num_samples\n",
    "print(mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T14:11:33.000284Z",
     "iopub.status.busy": "2025-01-02T14:11:32.999927Z",
     "iopub.status.idle": "2025-01-02T14:15:13.934685Z",
     "shell.execute_reply": "2025-01-02T14:15:13.933872Z",
     "shell.execute_reply.started": "2025-01-02T14:11:33.000247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809df8e9916949cba8827af49814b1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/125594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length =8192,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 150,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T14:15:13.935807Z",
     "iopub.status.busy": "2025-01-02T14:15:13.935581Z",
     "iopub.status.idle": "2025-01-02T16:08:20.201210Z",
     "shell.execute_reply": "2025-01-02T16:08:20.200378Z",
     "shell.execute_reply.started": "2025-01-02T14:15:13.935786Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 125,594 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 150\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 1:52:17, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.886200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.664100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.525100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.547400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.480700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.585300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.512100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.518800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.549100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.531200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.506100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.474800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.498800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.5688667964935302, metrics={'train_runtime': 6781.7813, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.022, 'total_flos': 9.251224049836032e+16, 'train_loss': 0.5688667964935302, 'epoch': 0.009554596557160374})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:08:20.202501Z",
     "iopub.status.busy": "2025-01-02T16:08:20.202161Z",
     "iopub.status.idle": "2025-01-02T16:08:20.787945Z",
     "shell.execute_reply": "2025-01-02T16:08:20.787221Z",
     "shell.execute_reply.started": "2025-01-02T16:08:20.202469Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.model',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:08:20.789461Z",
     "iopub.status.busy": "2025-01-02T16:08:20.789208Z",
     "iopub.status.idle": "2025-01-02T16:09:19.350426Z",
     "shell.execute_reply": "2025-01-02T16:09:19.349759Z",
     "shell.execute_reply.started": "2025-01-02T16:08:20.789441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.12: Fast Mistral patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load lora_model as a legacy tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "System: Answer the question correctly and accurately\n",
      "\n",
      "Instruction: How does the neurobiological process of synaptic plasticity contribute to the development and persistence of substance use disorders, particularly in the context of the reward system's dopamine pathways?\n",
      "\n",
      "Response: \n",
      "\n",
      "Synaptic plasticity refers to the ability of synapses, the junctions between neurons, to strengthen or weaken over time in response to increases or decreases in their activity. This process is crucial for learning and memory, as it allows the brain to adapt to new experiences and environments.\n",
      "\n",
      "In the context of substance use disorders, synaptic plasticity plays a significant role in the development and persistence of these disorders. Substances of abuse, such as cocaine, amphetamine, and nicotine, interact with the brain's reward system, which is primarily mediated by the neurotransmitter dopamine.\n",
      "\n",
      "When a person takes a drug, it interacts with dopamine receptors in the brain, leading to an increase in dopamine levels. This surge of dopamine triggers the reward system, which is responsible for feelings of pleasure and motivation. The brain then strengthens the synapses that were active during this rewarding experience, making it more likely that the person will repeat the behavior in the future. This is known as long-term potentiation (LTP) and is a form of synaptic plasticity.\n",
      "\n",
      "Over time, the brain becomes more sensitive to the drug, requiring higher doses to achieve the same level of pleasure. This is because the synapses that were strengthened during drug use have become more efficient at releasing dopamine, and the brain has adapted to this new level of dopamine release. This is known as tolerance.\n",
      "\n",
      "In addition, the brain also develops a negative feedback mechanism to counteract the effects of the drug. This is known as long-term depression (LTD) and involves weakening the synapses that were active during drug use. This can lead to a decrease in the rewarding effects of the drug and is thought to contribute to the development of withdrawal symptoms when the drug is not available.\n",
      "\n",
      "The persistence of substance use disorders is also influenced by synaptic plasticity. The brain's reward system becomes \"remodeled\" by the drug, with changes in the strength and number of synapses. These changes can persist long after the drug is no longer being used, leading to cravings and the compulsive desire to use the drug again.\n",
      "\n",
      "In summary, synaptic plasticity contributes to the development and persistence of substance use disorders by strengthening the synapses that are active during drug use, leading to tolerance and the need for higher doses, and by weakening the synapses that are active during withdrawal, leading to cravings and the compulsive desire to use the drug again. These changes in the brain's reward system can persist long after the drug is no longer being used, making it difficult for individuals to stop using substances of abuse.\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Answer the question correctly and accurately\", # instruction\n",
    "        \"How does the neurobiological process of synaptic plasticity contribute to the development and persistence of substance use disorders, particularly in the context of the reward system's dopamine pathways?\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
